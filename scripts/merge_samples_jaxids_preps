#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
File: merge_samples_jaxids_preps
Author: Benjamin Leopold
Date: 2016-08-05T12:59:11
Description: Tracks parents and children within jaxid hierarchy.
             Matches all sequence prep (dna, rna, 16s) with samples via jaxids.

Usage Example:
    merge_samples_jaxids_preps -l <jaxid_libaries.csv> -o outfile(stdout)

"""

# TODO: move from reference spreadsheet usage to direct jaxid_db checking

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Imports ~~~~~
from __future__ import print_function

import os
import re
import logging, time
import argparse
from collections import OrderedDict

from cutlass_utils import load_data, get_field_header, write_out_csv

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Constants ~~~~~
COOLNESS = True

class settings():
    """default values for  settings"""
    jaxid_ref_file = 'data_files/jaxid_database_export_20170217.csv'
    id_ref_map = OrderedDict()
    jaxid_sample_fieldname = 'jaxid_sample'
    jaxid_lib_fieldname = 'jaxid_library'
    parent_outfile = '_merged_parent_ids.csv'
    sample_outfile = '_merged_samples.csv'
    # filename_fields = ['dcc_file_base', 'dir', 'runid', 'original_file_base', 'second_file_base',
    #                    'final_sample_name', 'rand_subject_id', 'flag_meanings']
    filename_fields = ['dcc_file_base', 'dir', 'original_file_base', 'second_file_base',
                       'final_sample_name', 'rand_subject_id', 'flag_meanings']

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Functional ~~~~~
# Log It!
def log_it(logname=os.path.basename(__file__)):
    """log_it setup"""
    curtime = time.strftime("%Y%m%d-%H%M")
    logfile = '{}_{}.log'.format(curtime, logname)

    loglevel = logging.DEBUG
    logFormat = \
        "%(asctime)s %(levelname)5s: %(module)15s %(funcName)10s: %(message)s"
    formatter = logging.Formatter(logFormat)

    logging.basicConfig(format=logFormat)
    logger = logging.getLogger(logname)
    logger.setLevel(loglevel)

    ch = logging.StreamHandler()
    ch.setLevel(loglevel)
    ch.setFormatter(formatter)
    logger.addHandler(ch)

    fh = logging.FileHandler(logfile, mode='a')
    fh.setFormatter(formatter)
    logger.addHandler(fh)

    return logger

log = log_it('merge_samples')

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~   Local Functions   ~~~~~

def unique_order(items):
    """ keep only unique items from an incoming list, in order
       from Raymond Hettinger: http://stackoverflow.com/a/7961425/160063
    """
    from collections import OrderedDict
    return list(OrderedDict.fromkeys(items))


def merge_ordered_dicts(*dicts):
    """function to traverse items() of each dict as passed, in order.
       Then gather all from iterator into single var to return.
    """
    from itertools import chain
    chunk = OrderedDict()
    for (k,v) in chain.from_iterable([d.items() for d in dicts]):
        chunk[k] = v
    return chunk


def build_id_ref_map(ref_in):
    """construct reference mapping dict of ids from csv file"""
    log.debug("begin function")
    ref_map = {}
    for row in load_data(ref_in):
        # log.warning('row: %s', str(row))
        ref_map[row['jaxid']] = row
    log.debug("finished function")
    return ref_map


def which_id_type(record_dict):
    """determine which id type is being parsed"""
    # spreadsheet logic:
    #   =IF(NOT(ISERROR(search("pool",D73))),"pool",
    #       IF(AND(F73="Z",G73="Z",H73="Z"),"NULL",
    #           IF(and(G73="Z",H73="Z"),"specimen",
    #              IF(H73="Z","extraction",
    #                         "library")
    #           )
    #         )
    #      )
    log.debug("begin function")
    try:
        idtype = ''

        collab = record_dict['collab_id']
        sample = record_dict['sample']
        nucleic = record_dict['nucleic_acid']
        seqtype = record_dict['seq_type']

        if re.search('pool', collab):
            idtype = 'pool'
        elif sample == 'Z':
            idtype = None
        elif nucleic == 'Z':
            idtype = 'specimen'
        elif seqtype == 'Z':
            idtype = 'extraction'
        else:
            idtype = 'library'
        # log.debug("idtype==%s", idtype)
        return idtype
    except Exception, e:
        # raise e
        # log.exception("idtype==%s", idtype)
        return ''


def which_parent_type(id_type):
    """Goes up the hierarchical list of types
       returning the direct parent type.
    """
    log.debug("begin function")
    id_list = [None,
               'specimen',
               'extraction',
               'library',
              ]
    family_tree = {
            id_list[t-1]:id_list[t]
            for t in id_list
            if t > 0
            }
    return family_tree[id_type]


def get_received_id(idrow):
    """use get_parent_id to track recursively to original sample received"""
    log.debug("going up the heritage: %s", str(idrow))
    try:
        if idrow:
            parent_id = idrow['parent_jaxid']
            if parent_id == 'received':
                # log.debug("if1 parent_id: %s", parent_id)
                return idrow['jaxid']
            elif parent_id in settings.id_ref_map:
                # log.debug("elif2 parent_id: %s", parent_id)
                return get_received_id(settings.id_ref_map[parent_id])
        else:
            # log.debug("else parent_id: %s", parent_id)
            return ''

    except Exception, e:
        log.exception(str(e))
        raise e


def track_library_parents(library_csv, id_ref_map, jaxid_lib_field, map_output=False):
    """Incoming: csv file with library jaxids
       Outgoing: csv outfile with new column 'jaxid_parent'
       Uses the 'settings.jaxid_ref_file' as mapping source.
       Return: parent_id_map of all updated rows, for later use
       Option: write parent_map to csv outfile
    """
    log.debug("begin function")
    outfile = library_csv[:-4]+'_parents.csv'
    outfields = get_field_header(library_csv)
    outfields.insert(outfields.index(
                     jaxid_lib_field)+1,
                     'jaxid_received')
    write_out_csv(outfile, outfields) # write column headers

    parent_id_map = {}
    for row in load_data(library_csv):
        try:
            row['jaxid_received'] = ''
            if row[jaxid_lib_field] in id_ref_map:
                idrow = id_ref_map.get(row[jaxid_lib_field])
                # log.warning('idrow: %s', str(idrow))
                idtype = which_id_type(idrow)
                if not idtype:  # just in case there are not fields in idrow
                    idtype = 'library'
                try:
                    parent_id = get_received_id(idrow)
                    row['jaxid_received'] = parent_id
                except Exception, e:
                    raise e
            write_out_csv(outfile, outfields, [row])
            if row['jaxid_received'] in parent_id_map:
                parent_id_map[row['jaxid_received']].append(row)
            else:
                parent_id_map[row['jaxid_received']] = [row]
        except Exception, e:
            log.exception(str(e))
            raise e

    log.debug("finished function")
    return parent_id_map


def concat_prep_id(*pieces):
    """concat the group of args with '_'s into one "prep_id" field"""
    return '_'.join(pieces)


def merge_library_prep_ids_w_sample_list(sample_list_in,
                                         parent_id_map):
    """Usage: for seqprep rows with library jaxids matching to received ids
              with the list of samples nodes.
       Intent: create sheets of [dr]naprep node metadata.
       Tactic: Use double-for loop in each of samples and parent_id_map
    """
    log.debug("begin function")
    jaxid_sample_fieldname = settings.jaxid_sample_fieldname
    outfile = sample_list_in[:-4]+'_merged.csv'

    # initiate and extend outfields names
    outfields = get_field_header(sample_list_in)
    outfields.extend(['prep_id'])
    pv = parent_id_map.itervalues()
    outfields.extend(pv.next()[0].keys())
    log.debug("func: %s, fieldnames: %s",
              'merge_library_prep...',
              str(outfields))
    write_out_csv(outfile, outfields) # write column headers
    noprep_outfile = sample_list_in[:-4]+'_nopreps.csv'
    write_out_csv(noprep_outfile, outfields) # write column headers

    sample_list_dict = OrderedDict()

    for srow in load_data(sample_list_in):
        # log.debug("row: %s", srow)
        try:
            jaxid_sample = srow.get(jaxid_sample_fieldname)
            if jaxid_sample:
                log.debug("jaxid_sample: %s", jaxid_sample)
                if jaxid_sample in parent_id_map:
                    rand_sample_name = srow['rand_sample_name']
                    for libidrow in [row for row in
                            parent_id_map[jaxid_sample][:]]:
                        # log.debug('items: %s', str(libidrow))

                        # check if 'flags' field includes the 'publicized' tag,
                        # then format it to be 4 chars if not
                        if int(libidrow['flags']) < 120:
                            flags = int(libidrow['flags'])
                            flags += 120
                            # log.debug('flags: %s', flags)
                            libidrow['flags'] = '{:04d}'.format(flags)

                        srow['prep_id'] = concat_prep_id(
                                                srow['project_code'],
                                                srow['jaxid_sample'],
                                                libidrow['seq_type'],
                                                libidrow['sample_type_code'],
                                                libidrow['tech_rep'],
                                                libidrow['bio_rep'],
                                                libidrow['flags'],
                                                rand_sample_name,
                                                libidrow['run_id']
                                                )

                        # gather all values together
                        sampleidvals = OrderedDict()
                        sampleidvals = merge_ordered_dicts(srow, libidrow)
                        log.debug('idvals: %s', str(sampleidvals))
                        write_out_csv(outfile, outfields, [sampleidvals])
                else:
                    write_out_csv(noprep_outfile, outfields, [srow])

        except Exception, e:
            log.exception(str(e))
            raise e

    log.debug("finished function")
    return True


def build_sample_prep_map(sample_sheet_merged):
    """given a pre-merged sample sheet, glean out the jaxid and the prep_id"""
    sample_prep_map = {}
    for row in load_data(sample_sheet_merged):
        if row['jaxid_sample'] in sample_prep_map:
            sample_prep_map[row['jaxid_sample']].append(row['prep_id'])
        else:
            sample_prep_map[row['jaxid_sample']] = [row['prep_id']]
    return sample_prep_map


def match_base_filenames_prepids(filename_list, sample_sheet_merged):
    """Incoming: - list of file paths and notes;
                    fields: dir,original_file_base,second_file_base,flag_meanings
                 - sample sheet merged w/ prep_ids (previously merged)
       Return: write_out_csv list to use with 'dcc_submission_file_name_change'
               Fields: dcc_file_base,dir,original_file_base,second_file_base,
                       final_sample_name,rand_subject_id,flag_meanings
       Technique: for each row in filename_list
                    check filename contains Jaxid # and Runid
                    get runid from row[] field
                    get id_ref_map row for jaxid_library, get jaxid received
                    check sample_prep_map for record wih jaxid_recd
                    check match jaxid_library and runid match sample prep_id
    """
    log.debug("begin function")
    outfile = filename_list[:-4]+'_prepids.csv'
    outfields = settings.filename_fields
    outfields.extend(get_field_header(filename_list))
    # outfields = set(outfields) # only unique set of field headers
    outfields = unique_order(outfields)
    write_out_csv(outfile, outfields) # write column headers

    outnolibs = filename_list[:-4]+'_no_lib_jaxid.csv'
    write_out_csv(outnolibs, outfields) # write column headers

    sample_prep_map = build_sample_prep_map(sample_sheet_merged)

    filename_map = OrderedDict()
    for row in load_data(filename_list):
        log.info('next filename...')
        try:
            row['dcc_file_base'] = ''
            file_base = row['second_file_base'] \
                if row['second_file_base'] != '' \
                else row['original_file_base']

            # get library id from filename (IFF present!)
            jaxid_match = re.search('[-_](J[0-9]{5})',file_base)
            # runid_match =  re.search('_(A[A-Z0-9]{4})$',file_base) # match for MiSeq only!!
            # runid_match = re.search('[-_]([A-Z0-9]*)$',file_base) # match for any alphanumeric string at end of file_base!

            if jaxid_match: # and runid_match:
                jaxid_library = jaxid_match.group(1)
                # runid = runid_match.group(1)
                runid = row['run_id']
                log.info('Jaxid library in file_base: %s', jaxid_library)

                try:
                    idrow = settings.id_ref_map.get(jaxid_library)
                    # log.warning('idrow: %s', str(idrow))
                    parent_id = get_received_id(idrow)
                    log.debug('received parent_id: %s', parent_id)
                    if parent_id in sample_prep_map:
                        for sam_prep in sample_prep_map[parent_id][:]:
                            log.debug('sample_prep_map: %s', str(sam_prep))
                            smatch = re.search(parent_id,sam_prep)
                            # log.debug('smatch %s', str(smatch))
                            if re.search(parent_id,sam_prep) and \
                               re.search(runid,sam_prep):
                                log.info('matching prep_id %s', sam_prep)
                                row['dcc_file_base'] = sam_prep
                                # log.debug('prep_id: %s', sam_prep)
                                write_out_csv(outfile, outfields, [row])
                            else:
                                log.debug('no sample prep_id! ')
                    else:
                        log.warning('no parent_id (%s) in sample_prep_map?!', parent_id)
                        write_out_csv(outnolibs, outfields, [row])

                except Exception, e:
                    log.exception('WTF! jaxid: %s', jaxid_library)
                    raise e
            else:
                log.warning('no jaxid_match or no runid_match ?!: %s', file_base)
                write_out_csv(outnolibs, outfields, [row])

        except Exception, e:
            log.exception(str(e))
            raise e
    log.debug("finished function")
    return True


def match_file_checksums(fof, checksum_list, outfile):
    """match filenames to the checksum list via prep_id matching filename"""
    log.debug("begin function")
    spec_fieldnames = get_field_header(checksum_list)
    spec_fieldnames.extend(get_field_header(fof))
    outfile = fof[:-4] + outfile
    write_out_csv(outfile, spec_fieldnames) # write column headers
    try:
        for filespec in load_data(checksum_list):
            filebase = os.path.basename(filespec['local_file'])
            specfields = filespec
            for prep in load_data(fof):
                prep_id = prep.get('prep_id')
                if prep_id and re.search(prep_id, filebase):
                    specfields.update(prep)
                    write_out_csv(outfile, spec_fieldnames, [specfields])
                # else:
                #     log.warning('prep_id issues: %s', str(prep))
    except Exception, e:
        log.exception(str(e.message))
        raise e

    log.debug("finished function")
    return True


def match_file_checksums_dnapreps(dnapreps, checksum_list, outfile):
    """match dna prep metadata fields to the checksum list
       via prep_id matching filename,
       write_out_csv list of raw/clean files
    """
    log.debug("begin function")
    spec_fieldnames = get_field_header(dnapreps)
    spec_fieldnames.extend(get_field_header(checksum_list))
    outfile = dnapreps[:-4] + '_checksummed.csv'
    write_out_csv(outfile, spec_fieldnames) # write column headers
    try:
        for filespec in load_data(checksum_list):
            filebase = os.path.basename(filespec['local_file'])
            specfields = filespec
            matched = False
            for prep in load_data(dnapreps):
                prep_id = prep.get('prep_id')
                # log.debug('prep_id: %s', prep_id)
                prep_id = re.sub('-','_',prep_id)
                filebase = re.sub('-','_',filebase)
                # log.debug('prep_id: %s, filebase: %s', prep_id, filebase)

                if prep_id and filebase and re.search(prep_id, filebase):
                    specfields.update(prep)
                    log.debug('matching prep_id: %s, filebase: %s', prep_id, filebase)
                    write_out_csv(outfile, spec_fieldnames, [specfields])
                    matched = True
                # else:
                #     log.warning('prep_id (%s) and filebase (%s) do not match!', prep_id, filebase)
            if not matched:
                write_out_csv(outfile[:-4]+'_no_prep_match.csv',
                              spec_fieldnames, [specfields])
    except Exception, e:
        log.exception(str(e.message))
        raise e

    log.debug("finished function")
    return True


def create_filename_change_csv_format(filename):
    """modify format to match expected column names"""
    fieldnames = get_field_header(filename)
    field_header = settings.filename_fields
    # if set(field_header) != set(fieldnames):
    #     prepend_line(filename, )
    pass

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Make it happen! ~~~~~

def parse_args(args):
    """all args to script are parsed here"""
    #TODO: add inbound/outbound streams, not files (read/write csv and/or sql)
    parser = argparse.ArgumentParser()
    add = parser.add_argument

    # add('-j', '--jaxid_lib_fieldname',
    #     metavar='JAXID_LIB_FIELDNAME', default=settings,jaxid_lib_fieldname,
    #     help="field name of beginning jaxid column {}".format(
    #           settings.jaxid_lib_fieldname))
    add('-l', '--library_csv',
        metavar='LIBRARY_CSV', default=None,
        help="CSV File containing column {}".format(
            settings.jaxid_lib_fieldname))
    add('-m', '--map_output',  ## requires library_csv!!
        default=False, action='store_true',
        help="store parent jaxid results in csv outfile.")
    add('-s', '--sample_sheet',  ## requires library_csv!!
        metavar='SAMPLE_CSV', default=None,
        help="CSV File containing column {}".format(
            settings.jaxid_sample_fieldname))
    add('-f', '--file_name_changes',
        metavar='FILE_NAME_CHANGES', default=None,
        help="CSV File list of raw file names to match with prep_ids")
    add('-p', '--fof_pre_match_checksums',
        metavar='FOF_PRE_MATCH_CHECKSUMS', default=None,
        help="CSV File list of file names to match with checksums")
    add('-c', '--file_checksums',
        metavar='FILE_CHECKSUMS', default=None,
        help="CSV File list of raw file names with checksums and size")
    add('-d', '--file_dna_prep',
        metavar='FILE_DNA_PREP', default=None,
        help="CSV File list of dna library preps")
    add('-o', '--outfile',
        metavar='OUTFILE', default=settings.parent_outfile,
        help='File to store output')
    add("-r", "--jaxid_ref_file",
        metavar='JAXID_REF_FILE',
        default=settings.jaxid_ref_file,
        dest='ref_file',
        help="jaxid relation reference spreadsheet")
    add("-v", "--verbose",
        default=False,
        action="store_true",
        help="increase output verbosity")
    return parser.parse_args(args)


def main(args):
    """get it done!
       Use several times, sequentially...
           1) get source data and format spreadsheets
           2) sync dna prep and run info with master sample list
           3) match file names for external name changes (other script)
           4) sync dna prep and file checksum info lists
    """
    log.info('command line run: "%s"', ' '.join(sys.argv))

    success = False
    try:
        if args.verbose:
            log.setLevel(logging.DEBUG)
        else:
            log.setLevel(logging.INFO)

        if os.access(args.ref_file, os.R_OK):
            settings.id_ref_map = build_id_ref_map(args.ref_file)
        else:
            log.error("JAXid reference file is not readable or doesn't exist!")
            return False

        if args.library_csv:
            parent_id_map = track_library_parents(args.library_csv,
                                                  settings.id_ref_map,
                                                  settings.jaxid_lib_fieldname,
                                                  args.map_output
                                                 )
            success = True

        # presumes track_library_parents has been successfully (thus _merged.csv files exists!)
        if args.library_csv and args.sample_sheet:
            settings.sample_outfile = args.sample_sheet[:-4]+'_merged.csv'
            if os.access(settings.sample_outfile, os.W_OK):
                os.remove(settings.sample_outfile)
            success = merge_library_prep_ids_w_sample_list(
                args.sample_sheet,
                parent_id_map)

        # this step presumes the sample_sheet has been previously merged!
        if args.file_name_changes and args.sample_sheet:
            settings.sample_outfile = args.sample_sheet[:-4]+'_merged.csv'
            success = match_base_filenames_prepids(
                args.file_name_changes,
                settings.sample_outfile)

        # Do THIS in bash or vim or spreadsheet.concat instead!
        # if args.file_name_changes and args.create_namechange_format:
        #     success = True

        # prepare sheet for raw/etc file upload including path, size, checksums
        if args.fof_pre_match_checksums and args.file_checksums:
            match_file_checksums(
                args.fof_pre_match_checksums,
                args.file_checksums,
                args.outfile
                )

        # prepare sheet of raw/etc file info including rand_subject, checksum
        # info, etc, matched via prep_id
        if args.file_dna_prep and args.file_checksums:
            match_file_checksums_dnapreps(
                args.file_dna_prep,
                args.file_checksums,
                args.outfile
                )

    except Exception, e:
        raise e

    return success


if __name__ == '__main__':
    import sys
    # args = parse_args('-v -l test_jaxid_libs.csv'.split())
    args = parse_args(sys.argv[1:])
    sys.exit(main(args))
